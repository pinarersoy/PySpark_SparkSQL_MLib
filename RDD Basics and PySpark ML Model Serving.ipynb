{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: RDD Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz\n",
    "\n",
    "!tar xf spark-2.4.8-bin-hadoop2.7.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_configurations = (SparkConf().setMaster(\"local[*]\").setAppName(\"firstSparkSession\").set(\"spark.executor.memory\", \"2g\"))\n",
    "spark_context = SparkContext(conf = spark_configurations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieve SparkContext version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_context.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieve Python version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_context.pythonVer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Master URL to connect to**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_context.master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieves application name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'firstSparkSession'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_context.appName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieve application ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local-1623274365399'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_context.applicationId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Return default level of parallelism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_context.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Return default minimun number of partitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_context.defaultMinPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operations\n",
    "\n",
    "### Spark Actions\n",
    "#### Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing elements for collect: [4, 13, 13, 28, 36, 47, 56]\n"
     ]
    }
   ],
   "source": [
    "items = spark_context.parallelize ([4,13,13,28,36,47,56])\n",
    "\n",
    "number_list = items.collect()\n",
    "print (\"Printing elements for collect: %s\" % (number_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing first element with first operation of RDD: 4\n"
     ]
    }
   ],
   "source": [
    "items = spark_context.parallelize ([4,13,13,28,36,47,56])\n",
    "first_element = items.first()\n",
    "print (\"Printing first element with first operation of RDD: %s\" % (first_element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing specified number of elements with take operation of RDD: [4, 13, 13]\n"
     ]
    }
   ],
   "source": [
    "items = spark_context.parallelize ([4,13,13,28,36,47,56])\n",
    "take_element = items.take(3)\n",
    "print (\"Printing specified number of elements with take operation of RDD: %s\" % (take_element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[56, 13, 13, 13, 47]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = spark_context.parallelize ([4,13,13,28,36,47,56])\n",
    "items.takeSample(True, 5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take Ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44, 56, 131, 147, 836]\n"
     ]
    }
   ],
   "source": [
    "items = spark_context.parallelize ([44,131,836,147,56]).takeOrdered(6)\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing number of instances for count operation of RDD: 7\n"
     ]
    }
   ],
   "source": [
    "items = spark_context.parallelize ([4,13,13,28,36,47,56])\n",
    "element_count = items.count()\n",
    "print (\"Printing number of instances for count operation of RDD: %i\" % (element_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('first_num', 1), ('second_num', 2), ('third_num', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countKey = spark_context.parallelize([(\"first_num\", 300), (\"second_num\", 500), (\"third_num\", 900), (\"second_num\", 500), ])\n",
    "sorted(countKey.countByKey().items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GetPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing number of partitions for getNumPartitions operation of RDD: 3\n"
     ]
    }
   ],
   "source": [
    "partitioned = spark_context.parallelize ([4,13,13,28,36,47,56], 3)\n",
    "partition_list = partitioned.getNumPartitions() #List the number of partitions\n",
    "print (\"Printing number of partitions for getNumPartitions operation of RDD: %i\" % (partition_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing sum of items for sum operation of RDD: 197\n"
     ]
    }
   ],
   "source": [
    "summation_list = items.sum() \n",
    "print (\"Printing sum of items for sum operation of RDD: %i\" % (summation_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing grouped items for groupBy operation of RDD:  [(0, [4, 28, 36, 56]), (1, [13, 13, 47])]\n"
     ]
    }
   ],
   "source": [
    "grouped_list = items.groupBy(lambda x : x%2).mapValues(list).collect()\n",
    "print (\"Printing grouped items for groupBy operation of RDD: \", (grouped_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing repartitioned list for repartition operation of RDD:  [13, 28, 36, 4, 13, 56, 47]\n"
     ]
    }
   ],
   "source": [
    "repartitioned_list = items.repartition(4).collect()\n",
    "print (\"Printing repartitioned list for repartition operation of RDD: \", (repartitioned_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SaveAsTextFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_list= items.saveAsTextFile(\"items.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing mapped items for map operation of RDD:  [6, 15, 15, 30, 38, 49, 58]\n"
     ]
    }
   ],
   "source": [
    "items = spark_context.parallelize ([4,13,13,28,36,47,56])\n",
    "mapped_list = items.map(lambda x: x+2).collect()\n",
    "print (\"Printing mapped items for map operation of RDD: \", (mapped_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MapPartitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 167]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitioned = spark_context.parallelize ([4,13,13,28,36,47,56], 2)\n",
    "def mapPartitionFunc(ind): yield sum(ind)\n",
    "partitioned.mapPartitions(mapPartitionFunc).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MapPartitionsByIndex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitioned = spark_context.parallelize ([4,13,13,28,36,47,56], 4)\n",
    "def mapPartitionByIndexFunc(indSlicer, ind): yield indSlicer\n",
    "partitioned.mapPartitionsWithIndex(mapPartitionByIndexFunc).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing filtered list items for filter operation of RDD:  [4, 28, 36, 56]\n"
     ]
    }
   ],
   "source": [
    "items = spark_context.parallelize ([4,13,13,28,36,47,56])\n",
    "filtered_list = items.filter(lambda x: x % 2 == 0).collect()\n",
    "print (\"Printing filtered list items for filter operation of RDD: \", (filtered_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FlatMap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = spark_context.parallelize ([4,13,13,28,36,47,56])\n",
    "items.flatMap(lambda x: range(1, x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 5, 9, 10, 10, 11, 17, 19]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_items = spark_context.parallelize(range(20), 4)\n",
    "sampling_items.sample(True, 0.3).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('k', (98, 43)), ('k', (98, 120))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = spark_context.parallelize([(\"k\", 98), (\"m\", 65)])\n",
    "list2 = spark_context.parallelize([(\"k\", 120), (\"k\", 43)])\n",
    "sorted(list1.join(list2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Union**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_items = spark_context.parallelize(range(5), 2)\n",
    "union_items.union(union_items).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intersection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 14]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group1 = spark_context.parallelize([2, 10, 17, 3, 14, 5])\n",
    "group2 = spark_context.parallelize([2, 8, 5, 34, 42, 14])\n",
    "group1.intersection(group2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distinct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing distinct items for distinct operation of RDD:  [36, 13, 4, 28, 56, 47]\n"
     ]
    }
   ],
   "source": [
    "items = spark_context.parallelize ([4,13,13,28,36,47,56])\n",
    "unique_element_list = items.distinct().collect()\n",
    "print (\"Printing distinct items for distinct operation of RDD: \", (unique_element_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GroupByKey**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('first_num', 1), ('second_num', 1), ('third_num', 1)]\n",
      "[('first_num', [300]), ('second_num', [500]), ('third_num', [900])]\n"
     ]
    }
   ],
   "source": [
    "groupedKeys = spark_context.parallelize([(\"first_num\", 300), (\"second_num\", 500), (\"third_num\", 900)])\n",
    "\n",
    "print(sorted(groupedKeys.groupByKey().mapValues(len).collect()))\n",
    "print(sorted(groupedKeys.groupByKey().mapValues(list).collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ReduceByKey**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('first_num', 300), ('second_num', 0), ('third_num', 900)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import sub\n",
    "reducedKeys = spark_context.parallelize([(\"first_num\", 300), (\"second_num\", 500), (\"third_num\", 900), (\"second_num\", 500), ])\n",
    "sorted(reducedKeys.reduceByKey(sub).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AggregateByKey**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190, 20)\n",
      "PythonRDD[90] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "item_group1 = spark_context.parallelize([('first',5),('first',3),('second',3)])\n",
    "item_group2 = spark_context.parallelize(range(20))\n",
    "\n",
    "# Aggregate RDD elements of each partition and then the results\n",
    "firstGroup = (lambda x,y: (x[0]+y,x[1]+1))\n",
    "aggregatedGroup = (lambda x,y:(x[0]+y[0],x[1]+y[1]))\n",
    "\n",
    "print(item_group2.aggregate((0,0),firstGroup,aggregatedGroup))\n",
    "\n",
    "#Aggregate values of each RDD key.collect()\n",
    "print(item_group1.aggregateByKey((0,0),firstGroup,aggregatedGroup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SortByKey**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fifth', 58)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_list = [('first', 7), ('second', 9), ('third', 11), ('fourth', 34), ('fifth', 58)]\n",
    "spark_context.parallelize(item_list).sortByKey().first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[109] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_list = spark_context.parallelize([('first',5),('first',3),('second',3)])\n",
    "\n",
    "item_list.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[109] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_list.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_list.persist(pyspark.StorageLevel.MEMORY_AND_DISK )\n",
    "item_list.getStorageLevel()\n",
    "print(item_list.getStorageLevel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_list.persist(pyspark.StorageLevel.MEMORY_AND_DISK_2 )\n",
    "item_list.getStorageLevel()\n",
    "print(item_list.getStorageLevel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Spark ML Model Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"FirstSparkApplication\").config (\"spark.executor.memory\", \"8g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", \"true\").load('dataset/titanic_train.csv')\n",
    "test_dataset = spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", \"true\").load('dataset/titanic_test.csv')\n",
    "\n",
    "training_dataset.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Row Count\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training Dataset Row Count\")\n",
    "training_dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Passenger Counts\n",
      "+------------------+\n",
      "|count(PassengerId)|\n",
      "+------------------+\n",
      "|               891|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique Passenger Counts\")\n",
    "training_dataset.agg(countDistinct(\"PassengerId\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Row Count\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test Dataset Row Count\")\n",
    "test_dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------------------------\n",
      " PassengerId | 1                                                   \n",
      " Survived    | 0                                                   \n",
      " Pclass      | 3                                                   \n",
      " Name        | Braund, Mr. Owen Harris                             \n",
      " Sex         | male                                                \n",
      " Age         | 22.0                                                \n",
      " SibSp       | 1                                                   \n",
      " Parch       | 0                                                   \n",
      " Ticket      | A/5 21171                                           \n",
      " Fare        | 7.25                                                \n",
      " Cabin       | null                                                \n",
      " Embarked    | S                                                   \n",
      "-RECORD 1----------------------------------------------------------\n",
      " PassengerId | 2                                                   \n",
      " Survived    | 1                                                   \n",
      " Pclass      | 1                                                   \n",
      " Name        | Cumings, Mrs. John Bradley (Florence Briggs Thayer) \n",
      " Sex         | female                                              \n",
      " Age         | 38.0                                                \n",
      " SibSp       | 1                                                   \n",
      " Parch       | 0                                                   \n",
      " Ticket      | PC 17599                                            \n",
      " Fare        | 71.2833                                             \n",
      " Cabin       | C85                                                 \n",
      " Embarked    | C                                                   \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_dataset.show(2, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------\n",
      " summary     | count               \n",
      " PassengerId | 891                 \n",
      " Survived    | 891                 \n",
      " Pclass      | 891                 \n",
      " Name        | 891                 \n",
      " Sex         | 891                 \n",
      " Age         | 714                 \n",
      " SibSp       | 891                 \n",
      " Parch       | 891                 \n",
      " Ticket      | 891                 \n",
      " Fare        | 891                 \n",
      " Cabin       | 204                 \n",
      " Embarked    | 889                 \n",
      "-RECORD 1--------------------------\n",
      " summary     | mean                \n",
      " PassengerId | 446.0               \n",
      " Survived    | 0.3838383838383838  \n",
      " Pclass      | 2.308641975308642   \n",
      " Name        | null                \n",
      " Sex         | null                \n",
      " Age         | 29.69911764705882   \n",
      " SibSp       | 0.5230078563411896  \n",
      " Parch       | 0.38159371492704824 \n",
      " Ticket      | 260318.54916792738  \n",
      " Fare        | 32.2042079685746    \n",
      " Cabin       | null                \n",
      " Embarked    | null                \n",
      "-RECORD 2--------------------------\n",
      " summary     | stddev              \n",
      " PassengerId | 257.3538420152301   \n",
      " Survived    | 0.48659245426485753 \n",
      " Pclass      | 0.8360712409770491  \n",
      " Name        | null                \n",
      " Sex         | null                \n",
      " Age         | 14.526497332334035  \n",
      " SibSp       | 1.1027434322934315  \n",
      " Parch       | 0.8060572211299488  \n",
      " Ticket      | 471609.26868834975  \n",
      " Fare        | 49.69342859718089   \n",
      " Cabin       | null                \n",
      " Embarked    | null                \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_dataset.describe().show(3,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values\n",
      "\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|  0|    0|    0|     0|   0|    0|       0|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n",
      "Null values\n",
      "\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n",
      "Not Null values\n",
      "\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|        891|     891|   891| 891|891|714|  891|  891|   891| 891|  204|     889|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting the number of null values\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "print (\"NaN values\\n\")\n",
    "training_dataset.select([count(when(isnan(item), item)).alias(item) for item in training_dataset.columns]).show(5)\n",
    "\n",
    "print (\"Null values\\n\")\n",
    "training_dataset.select([count(when(col(item).isNull(), item)).alias(item) for item in training_dataset.columns]).show(5)\n",
    "\n",
    "print (\"Not Null values\\n\")\n",
    "training_dataset.select([count(when(col(item).isNotNull(), item)).alias(item) for item in training_dataset.columns]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming Column Name\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, PassengerClasses: int, Name: string, Gender: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Renaming Column Name\")\n",
    "training_dataset = training_dataset.withColumnRenamed(\"Pclass\",\"PassengerClasses\").withColumnRenamed(\"Sex\",\"Gender\")\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting the number of Passenger per Classes\n",
      "+----------------+-----+\n",
      "|PassengerClasses|count|\n",
      "+----------------+-----+\n",
      "|               1|  216|\n",
      "|               2|  184|\n",
      "|               3|  491|\n",
      "+----------------+-----+\n",
      "\n",
      "Counting the number of Survivals by Classes\n",
      "+----------------+------+--------+-----+\n",
      "|PassengerClasses|Gender|Survived|count|\n",
      "+----------------+------+--------+-----+\n",
      "|               1|female|       0|    3|\n",
      "|               1|female|       1|   91|\n",
      "|               1|  male|       0|   77|\n",
      "|               1|  male|       1|   45|\n",
      "|               2|female|       0|    6|\n",
      "|               2|female|       1|   70|\n",
      "|               2|  male|       0|   91|\n",
      "|               2|  male|       1|   17|\n",
      "|               3|female|       0|   72|\n",
      "|               3|female|       1|   72|\n",
      "|               3|  male|       0|  300|\n",
      "|               3|  male|       1|   47|\n",
      "+----------------+------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Counting the number of Passenger per Classes\")\n",
    "training_dataset.groupBy(\"PassengerClasses\").count().sort(\"PassengerClasses\").show()\n",
    "\n",
    "\n",
    "print(\"Counting the number of Survivals by Classes\")\n",
    "training_dataset.groupBy(\"PassengerClasses\",\n",
    "                         \"Gender\",\n",
    "                         \"Survived\").count().sort(\"PassengerClasses\",\n",
    "                                                  \"Gender\",\n",
    "                                                  \"Survived\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting the number of Survivals by Classes\n",
      "+----------------+------+--------+-----+\n",
      "|PassengerClasses|Gender|Survived|count|\n",
      "+----------------+------+--------+-----+\n",
      "|               1|female|       0|    3|\n",
      "|               1|female|       1|   91|\n",
      "|               1|  male|       0|   77|\n",
      "|               1|  male|       1|   45|\n",
      "|               2|female|       0|    6|\n",
      "|               2|female|       1|   70|\n",
      "|               2|  male|       0|   91|\n",
      "|               2|  male|       1|   17|\n",
      "|               3|female|       0|   72|\n",
      "|               3|female|       1|   72|\n",
      "|               3|  male|       0|  300|\n",
      "|               3|  male|       1|   47|\n",
      "+----------------+------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Counting the number of Survivals by Classes\")\n",
    "training_dataset.groupBy(\"PassengerClasses\",\n",
    "                         \"Gender\",\n",
    "                         \"Survived\").count().sort(\"PassengerClasses\",\n",
    "                                                  \"Gender\",\n",
    "                                                  \"Survived\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                Name| Title|\n",
      "+--------------------+------+\n",
      "|Braund, Mr. Owen ...|    Mr|\n",
      "|Cumings, Mrs. Joh...|   Mrs|\n",
      "|Heikkinen, Miss. ...|  Miss|\n",
      "|Futrelle, Mrs. Ja...|   Mrs|\n",
      "|Allen, Mr. Willia...|    Mr|\n",
      "|    Moran, Mr. James|    Mr|\n",
      "|McCarthy, Mr. Tim...|    Mr|\n",
      "|Palsson, Master. ...|Master|\n",
      "|Johnson, Mrs. Osc...|   Mrs|\n",
      "|Nasser, Mrs. Nich...|   Mrs|\n",
      "+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_dataset = training_dataset.withColumn(\"Title\", regexp_extract(col(\"Name\"),\"([A-Za-z]+)\\.\", 1))\n",
    "training_dataset.select(\"Name\",\"Title\").show(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|   Title|count|\n",
      "+--------+-----+\n",
      "|     Don|    1|\n",
      "|    Miss|  182|\n",
      "|Countess|    1|\n",
      "|     Col|    2|\n",
      "|     Rev|    6|\n",
      "|    Lady|    1|\n",
      "|  Master|   40|\n",
      "|     Mme|    1|\n",
      "|    Capt|    1|\n",
      "|      Mr|  517|\n",
      "|      Dr|    7|\n",
      "|     Mrs|  125|\n",
      "|     Sir|    1|\n",
      "|Jonkheer|    1|\n",
      "|    Mlle|    2|\n",
      "|   Major|    2|\n",
      "|      Ms|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_dataset.groupBy(\"Title\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|  Title|count|\n",
      "+-------+-----+\n",
      "|     Mr|  517|\n",
      "|   Miss|  185|\n",
      "|    Mrs|  126|\n",
      "|Royalty|   45|\n",
      "| Ranked|   18|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_df = training_dataset.\\\n",
    "replace([\"Mme\", \n",
    "         \"Mlle\",\"Ms\",\n",
    "         \"Major\",\"Dr\", \"Capt\",\"Col\",\"Rev\",\n",
    "         \"Lady\",\"Dona\", \"the Countess\",\"Countess\", \"Don\", \"Sir\", \"Jonkheer\",\"Master\"],\n",
    "        [\"Mrs\", \n",
    "         \"Miss\", \"Miss\",\n",
    "         \"Ranked\",\"Ranked\",\"Ranked\",\"Ranked\",\"Ranked\",\n",
    "         \"Royalty\",\"Royalty\",\"Royalty\",\"Royalty\",\"Royalty\",\"Royalty\",\"Royalty\",\"Royalty\"])\n",
    "\n",
    "feature_df.groupBy(\"Title\").count().sort(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PassengerId', 'int'),\n",
       " ('Survived', 'int'),\n",
       " ('PassengerClasses', 'int'),\n",
       " ('Name', 'string'),\n",
       " ('Gender', 'string'),\n",
       " ('Age', 'double'),\n",
       " ('SibSp', 'int'),\n",
       " ('Parch', 'int'),\n",
       " ('Ticket', 'string'),\n",
       " ('Fare', 'double'),\n",
       " ('Cabin', 'string'),\n",
       " ('Embarked', 'string'),\n",
       " ('Title', 'string')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Survived', 'int'),\n",
       " ('PassengerClasses', 'int'),\n",
       " ('SibSp', 'int'),\n",
       " ('Parch', 'int')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = feature_df.select(\n",
    "    \"Survived\",\n",
    "    \"PassengerClasses\",\n",
    "    \"SibSp\",\n",
    "    \"Parch\")\n",
    "\n",
    "df = df.dropna()\n",
    "df = df.fillna(0)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "parchIndexer = StringIndexer(inputCol=\"Parch\", outputCol=\"Parch_Ind\").fit(df)\n",
    "sibspIndexer = StringIndexer(inputCol=\"SibSp\", outputCol=\"SibSp_Ind\").fit(df)\n",
    "passangerIndexer = StringIndexer(inputCol=\"PassengerClasses\", outputCol=\"PassengerClasses_Ind\").fit(df)\n",
    "survivedIndexer = StringIndexer(inputCol=\"Survived\", outputCol=\"Survived_Ind\").fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "  inputCols = [\"PassengerClasses\",\"SibSp\",\"Parch\"],\n",
    "  outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier_c34a79a4a517"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"Survived\")\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_a32dcc338182"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, classifier, parchIndexer, sibspIndexer, passangerIndexer, survivedIndexer])\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training with ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent='DecisionTreeClassifier_c34a79a4a517', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       "  Param(parent='DecisionTreeClassifier_c34a79a4a517', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25},\n",
       " {Param(parent='DecisionTreeClassifier_c34a79a4a517', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       "  Param(parent='DecisionTreeClassifier_c34a79a4a517', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30},\n",
       " {Param(parent='DecisionTreeClassifier_c34a79a4a517', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10,\n",
       "  Param(parent='DecisionTreeClassifier_c34a79a4a517', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25},\n",
       " {Param(parent='DecisionTreeClassifier_c34a79a4a517', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10,\n",
       "  Param(parent='DecisionTreeClassifier_c34a79a4a517', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30},\n",
       " {Param(parent='DecisionTreeClassifier_c34a79a4a517', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 15,\n",
       "  Param(parent='DecisionTreeClassifier_c34a79a4a517', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25},\n",
       " {Param(parent='DecisionTreeClassifier_c34a79a4a517', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 15,\n",
       "  Param(parent='DecisionTreeClassifier_c34a79a4a517', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30},\n",
       " {Param(parent='DecisionTreeClassifier_c34a79a4a517', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 20,\n",
       "  Param(parent='DecisionTreeClassifier_c34a79a4a517', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25},\n",
       " {Param(parent='DecisionTreeClassifier_c34a79a4a517', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 20,\n",
       "  Param(parent='DecisionTreeClassifier_c34a79a4a517', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "  .addGrid(classifier.maxDepth, [5, 10, 15, 20]) \\\n",
    "  .addGrid(classifier.maxBins, [25, 30]) \\\n",
    "  .build()\n",
    "paramGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainValidationSplit_e439011fad8f"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvs = TrainValidationSplit(\n",
    "  estimator=pipeline,\n",
    "  estimatorParamMaps=paramGrid,\n",
    "  evaluator=MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"),\n",
    "  trainRatio=0.8)\n",
    "\n",
    "tvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = df.randomSplit([0.8, 0.2], seed = 345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "#### Print accuracy results by each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7014687100893997,\n",
       "  {Param(parent='DecisionTreeClassifier_d5f6b1ac3be4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       "   Param(parent='DecisionTreeClassifier_d5f6b1ac3be4', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 251}),\n",
       " (0.7014687100893997,\n",
       "  {Param(parent='DecisionTreeClassifier_d5f6b1ac3be4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       "   Param(parent='DecisionTreeClassifier_d5f6b1ac3be4', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 300}),\n",
       " (0.6830897733770511,\n",
       "  {Param(parent='DecisionTreeClassifier_d5f6b1ac3be4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10,\n",
       "   Param(parent='DecisionTreeClassifier_d5f6b1ac3be4', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 251}),\n",
       " (0.6830897733770511,\n",
       "  {Param(parent='DecisionTreeClassifier_d5f6b1ac3be4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10,\n",
       "   Param(parent='DecisionTreeClassifier_d5f6b1ac3be4', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 300}),\n",
       " (0.6830897733770511,\n",
       "  {Param(parent='DecisionTreeClassifier_d5f6b1ac3be4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 15,\n",
       "   Param(parent='DecisionTreeClassifier_d5f6b1ac3be4', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 251}),\n",
       " (0.6830897733770511,\n",
       "  {Param(parent='DecisionTreeClassifier_d5f6b1ac3be4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 15,\n",
       "   Param(parent='DecisionTreeClassifier_d5f6b1ac3be4', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 300}),\n",
       " (0.6830897733770511,\n",
       "  {Param(parent='DecisionTreeClassifier_d5f6b1ac3be4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 20,\n",
       "   Param(parent='DecisionTreeClassifier_d5f6b1ac3be4', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 251}),\n",
       " (0.6830897733770511,\n",
       "  {Param(parent='DecisionTreeClassifier_d5f6b1ac3be4', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 20,\n",
       "   Param(parent='DecisionTreeClassifier_d5f6b1ac3be4', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 300})]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(model.validationMetrics, model.getEstimatorParamMaps()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Serving with MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow import spark\n",
    "with mlflow.start_run(): \n",
    "    model = tvs.fit(train) \n",
    "    mlflow.spark.log_model(model, \"sparkML-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+-----+----------+\n",
      "|PassengerClasses|SibSp|Parch|Inferences|\n",
      "+----------------+-----+-----+----------+\n",
      "|1               |0    |0    |0.39849624|\n",
      "|1               |0    |0    |0.39849624|\n",
      "|1               |0    |0    |0.39849624|\n",
      "|1               |0    |0    |0.39849624|\n",
      "|1               |0    |0    |0.39849624|\n",
      "|1               |0    |0    |0.39849624|\n",
      "|1               |0    |0    |0.39849624|\n",
      "|1               |0    |0    |0.39849624|\n",
      "|1               |0    |0    |0.39849624|\n",
      "|1               |1    |0    |0.39849624|\n",
      "|1               |1    |0    |0.39849624|\n",
      "|1               |1    |2    |0.21428572|\n",
      "|2               |0    |0    |0.6545454 |\n",
      "|2               |0    |0    |0.6545454 |\n",
      "|2               |0    |0    |0.6545454 |\n",
      "|2               |0    |0    |0.6545454 |\n",
      "|2               |0    |0    |0.6545454 |\n",
      "|2               |0    |0    |0.6545454 |\n",
      "|2               |0    |0    |0.6545454 |\n",
      "|2               |1    |0    |0.6545454 |\n",
      "+----------------+-----+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mlflow.pyfunc\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "train.toPandas().to_csv('dataset.csv')\n",
    "\n",
    "model_path = '/Users/ersoyp/Documents/LAYER/ServingModelsWithApacheSpark/Scripts/mlruns/1/51ef199ab3b945e8a31b47cdfbf60912/artifacts/sparkML-model'\n",
    "titanic_path = '/Users/ersoyp/Documents/LAYER/ServingModelsWithApacheSpark/Scripts/dataset.csv'\n",
    "titanic_udf = mlflow.pyfunc.spark_udf(spark, model_path)\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", \"true\").option('delimiter', ';').load(titanic_path)\n",
    "\n",
    "columns = ['PassengerClasses', 'SibSp', 'Parch']\n",
    "          \n",
    "df.withColumn('Inferences', titanic_udf(*columns)).show(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
